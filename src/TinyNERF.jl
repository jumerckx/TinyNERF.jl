module TinyNERF

include("cumprod_adjoint.jl")
include("data.jl")

using Flux, CUDA, EllipsisNotation, Statistics, StatsBase, JSON, Images
using Flux: @nograd, Zygote.ignore, params, glorot_uniform

function posenc(x, L_embed)
    out = similar(x, (3 + 3 * 2 * L_embed, size(x)[2:end]...))
    out[1:3, ..] .= x
    out[4:end, ..] .= vcat((f.(2^i .* x) for i in 0:(L_embed - 1) for f in (sin, cos))...)

    return out
end
@nograd posenc

shifted_softplus(x) = softplus(x-1)

widened_sigmoid(x, Ïµ) = (1+2Ïµ)*sigmoid(x)-Ïµ
widened_sigmoid(x) = widened_sigmoid(x, eltype(x)(0.001))

struct MLP{T}
    arch::T
end
Flux.@functor MLP

function MLP(W::Integer=256, size_pos=96, size_dir=3*(1+2*4))
    arch = Chain(
        SkipConnection(
            Chain(
                Dense(size_pos, W, relu),
                Dense(W, W, relu),
                Dense(W, W, relu),
                Dense(W, W, relu), 
                Dense(W, W, relu)),
            (mx, x) -> vcat(mx, x)),
        Dense(W + size_pos, W, relu),
        Dense(W, W, relu),
        Dense(W, W, relu),
        Dense(W, W, identity),
        Dense(W-1+size_dir, WÃ·2, relu),
        Dense(WÃ·2, 3, widened_sigmoid))
    return MLP(arch)
end

function (m::MLP)(ð±, normed_directions)
    x = reshape(ð±, (size(ð±, 1), :))
    x = m.arch[1:5](x)
    Ïƒ = reshape(shifted_softplus.(x[1, :]), (1, size(ð±, 2), size(ð±, 3)) )
    x = vcat(x[2:end, :], reshape(normed_directions, (size(normed_directions, 1), :) ))
    x = m.arch[6:7](x)
    rgb = reshape(x, (3, size(ð±, 2), size(ð±, 3)) )

    return rgb, Ïƒ
end

"""
    get_rays(pixels, H, W, focal, cam2world)

Get origin and directions of rays through `pixels`.
`pixels` should be a vector of CartesianIndices.
"""
function get_rays(pixels, H, W, focal, cam2world)
    origin = similar(cam2world, 3)
    directions = similar(cam2world, (3, length(pixels)))

    directions[1, :] = (getindex.(pixels, 2) .- W/2 .- 1/2) ./ focal
    directions[2, :] = -(getindex.(pixels, 1) .- H/2 .- 1/2) ./ focal
    directions[3, :] .= -1
    directions = cam2world[1:3, 1:3] * directions

    origin = cam2world[1:3, 4]

    return origin, directions
end

gaussian_2d(x, y, Ïƒx, Ïƒy, W, H) = 1/(2Ï€*Ïƒx*Ïƒy)*exp(-(((x-W/2)/Ïƒx)^2+((y-H/2)/Ïƒy)^2) / 2)

function sample_t(directions, near, far, n_samples; randomized=true)
    t = reshape(range(near, stop=far, length=n_samples+2)[1:end-1], (1, 1, :))
    if randomized
        t = t .+ rand(1, size(directions, 2), n_samples+1) .* ((far - near)/(n_samples+1))
    end
    return t
end

function sample_t(directions::CuArray{T}, near, far, n_samples; randomized=true) where T
    t = CuArray(reshape(range(T(near), stop=far, length=n_samples+2)[1:end-1], (1, 1, :)))
    if randomized
        return t .+ (CUDA.rand(1, size(directions, 2), n_samples+1) .* T((far - near)/(n_samples+1)))
    end
    return t
end

function resample_t(t_vals, weights, n_samples; randomized=true, Î±=0.01)
    Î± = eltype(weights)(Î±)

    weights_pad = cat(weights[:, :, 1], weights, weights[:, :, end], dims=3) # 1 Ã— n_rays Ã— n_bins+2 (n_bins is the number of samples in the coarse pass.)
    weights_max = max.(weights_pad[:, :, 1:end-1], weights_pad[:, :, 2:end]) # 1 Ã— n_rays Ã— n_bins+1
    weights_blur = (weights_max[:, :, 1:end-1] + weights_max[:, :, 2:end]) ./ 2 # 1 Ã— n_rays Ã— n_bins
    weights = weights_blur .+ Î±

    Ïµ = eltype(weights)(1e-5)
    weight_sum = sum(weights, dims=3)
    padding = max.(0, Ïµ .- weight_sum)
    weights .+= padding ./ size(weights, 3)
    weight_sum += padding
    cdf = weights ./ weight_sum
    cdf = min.(1, cumsum(cdf[:, :, 1:end-1], dims=3)) # 1 Ã— n_rays Ã— n_bins-1
    cdf = cat(CUDA.zeros((1, size(cdf, 2))),
        cdf,
        CUDA.ones((1, size(cdf, 2))), dims=3) # 1 Ã— n_rays Ã— n_bins+1
    
    if randomized
        u = range(eltype(weights)(0), stop=1, length=n_samples+2)[1:end-1]
        u = reshape(u, (1, 1, 1, n_samples+1))
        u = u .+ CUDA.rand(1, size(weights, 2), 1, n_samples+1) .* (eltype(weights)(1/(n_samples+1)) - Ïµ)
    else
        u = reshape(range(eltype(weights)(0), stop=1-Ïµ, length=n_samples+1), (1, 1, 1, n_samples+1))
    end

    bin_indices = sum(u .>= cdf, dims=3)[:, :, 1, :] # 1 Ã— n_rays Ã— n_samples

    I = CartesianIndices((size(bin_indices)[1:2]..., 1)) .- CartesianIndex((0, 0, 1))

    cdf_interval = cdf[I .+ CartesianIndex.(0, 0, bin_indices)], cdf[I .+ CartesianIndex.(0, 0, bin_indices .+ 1)]
    bins_interval = t_vals[I .+ CartesianIndex.(0, 0, bin_indices)], t_vals[I .+ CartesianIndex.(0, 0, bin_indices .+ 1)]

    t = (dropdims(u, dims=3) .- cdf_interval[1]) ./ (cdf_interval[2] .- cdf_interval[1])
    return bins_interval[1] .+ t .* (bins_interval[2] .- bins_interval[1])
end

function cast(origin, directions, rÌ‡, t)
    T = eltype(t)

    tâ‚€, tâ‚ = t[:, :, 1:(end-1)], t[:, :, 2:end]
    midpoint, halfwidth = (tâ‚€ .+ tâ‚)./2, (tâ‚ .- tâ‚€)./2
    

    Î¼â‚œ = @. midpoint + (2*midpoint * (halfwidth^2)) / (3*(midpoint^2) + halfwidth^2)

    # https://github.com/JuliaGPU/CUDA.jl/issues/1044
    # varâ‚œ = @. (halfwidth^2)/3 -  T(4/15) * ((halfwidth^4) * (12*(midpoint^2) - halfwidth^2) / (3*(midpoint^2) + halfwidth^2)^2)
    # varáµ£ = @. rÌ‡^2 * ((midpoint^2)/4 + T(5/12)*(halfwidth^2) - T(4/15)*(halfwidth^4) / (3*midpoint^2 + halfwidth^2))
    
    varâ‚œ = (halfwidth.^2)./3 .-  T(4/15) .* ((halfwidth.^4) .* (12 .*(midpoint.^2) .- halfwidth.^2) ./ (3 .*(midpoint.^2) + halfwidth.^2).^2)
    varáµ£ = T(rÌ‡) .^ 2 .* ((midpoint .^ 2)./4 .+ T(5/12).*(halfwidth .^ 2) .- T(4/15).*(halfwidth .^ 4) ./ (3 .* (midpoint .^ 2)  .+ halfwidth .^ 2))

    Î¼ = @. origin + Î¼â‚œ * directions
    Î£_diag = varâ‚œ .* (directions .^ 2) .+ varáµ£ .* ((1 .- (directions .^ 2)) ./ sum(directions .^ 2, dims=1))
    
    normed_directions = similar(Î¼)
    normed_directions .= directions ./ sqrt.(sum(directions .^ 2, dims=1))

    return Î¼, Î£_diag, normed_directions
end

function IPE(Î¼, Î£_diag, degree=16)
    elt = eltype(Î£_diag)

    Î¼áµ§ = reduce(vcat, [2^i .* Î¼ for i in 0:degree-1])
    Î£áµ§_diag = reduce(vcat, [exp.((elt(-1/2) * 4^i) .* Î£_diag) for i in 0:degree-1])

    return [sin.(Î¼áµ§) .* Î£áµ§_diag; cos.(Î¼áµ§) .* Î£áµ§_diag]
end
@nograd IPE

function render(rgb, Ïƒ, t, directions)
    Î´, midpoint, Ïµ = ignore() do 
        tâ‚€, tâ‚ = t[:, :, 1:(end-1)], t[:, :, 2:end]
        midpoint, distance = (tâ‚€ .+ tâ‚)./2, (tâ‚ .- tâ‚€) # 1 Ã— n_rays Ã— (1 or n_samples)

        Î´ = distance .* sqrt.(sum(directions .^ 2, dims=1)) # 1 Ã— n_rays Ã— n_samples

        Ïµ = eps(eltype(Ïƒ))
        return Î´, midpoint, Ïµ
    end

    Î± = 1 .- exp.(-Ïƒ .* Î´) # 1 Ã— n_rays Ã— n_samples
    weights = Î± .* cat(gpu(ones(1, size(Î±, 2))), cumprod((1 .- Î± .+ Ïµ)[:, :, 1:end-1], dims=3), dims=3) # 1 Ã— n_rays Ã— n_samples

    rgb = sum(weights .* rgb, dims=3) # 3 Ã— n_rays Ã— 1
    depth = sum(weights .* midpoint, dims=3) # 1 Ã— n_rays Ã— 1
    acc = sum(weights, dims=3) # 1 Ã— n_rays Ã— 1

    return rgb, depth, acc, weights
end

# function train!(mlp, opt, images, poses, focal, rÌ‡, testimg, testpose; n_samples=128, batch_size=1024, n_iters=1000, i_plot=25)
#     H, W = size(images)[[2, 3]]
#     ps = params(mlp)
#     local â„’
    
#     for i in 1:n_iters
#         CUDA.@time begin
#             img_index = rand(1:size(images, 4))
#             pose = gpu(poses[:, :, img_index])

#             weights = ProbabilityWeights(map((x)->gaussian_2d(x..., W/6, H/6, W, H), Tuple.(CartesianIndices((H, W))))[:])

#             pixels = CartesianIndices((H, W))[sample(1:H*W, weights, batch_size, replace=false)]

#             # pixels = CartesianIndices((H, W))[rand(1:H*W, batch_size)]

#             target = gpu(images[:, :, :, img_index][:, pixels])

#             origin, directions = get_rays(pixels, H, W, focal, pose)
#             t = sample_t(directions, 2, 5, n_samples, randomized=true)
#             Î¼, Î£_diag, normed_directions = cast(origin, directions, rÌ‡, t)
            
#             gs = gradient(ps) do
#                 rgb, Ïƒ = mlp(IPE(Î¼, Î£_diag), posenc(normed_directions, 4))
#                 rgb, depth, acc, weights = render(rgb, Ïƒ, t, directions)
#                 â„’ = mean((rgb .- target).^2)
#                 return â„’
#             end

#             Flux.update!(opt, ps, gs)
#         end
#         # println(i)
#         if i%i_plot == 0
#             rgbs = []
#             for chunk in 0:ceil(Integer, (H*W)/batch_size)-1
#                 pixels = CartesianIndices((H, W))[chunk*batch_size+1:min(end, (chunk+1)*batch_size)]
#                 origin, directions = get_rays(pixels, H, W, focal, gpu(testpose))
#                 t = sample_t(directions, 2, 5, n_samples, randomized=true)
#                 Î¼, Î£_diag, normed_directions = cast(origin, directions, rÌ‡, t)
            
#                 rgb, Ïƒ = mlp(IPE(Î¼, Î£_diag), posenc(normed_directions, 4))
#                 rgb, depth, acc, weights = render(rgb, Ïƒ, t, directions)
#                 push!(rgbs, rgb)
#             end
#             rgb = reshape(reduce(hcat, rgbs), (3, H, W))
#             display(colorview(RGB, cpu(rgb)))
#             testing_loss = mean((rgb .- testimg) .^ 2)
#             PSNR = -10 * log10(testing_loss)
#             println("training loss: $â„’\ttesting loss: $testing_loss\tPSNR: $PSNR")
#         end
#     end
# end

function train!(mlp, opt, images, poses, focal, rÌ‡, testimg, testpose; n_samples=128, batch_size=1024, n_iters=1000, i_plot=25)
    H, W = size(images)[[2, 3]]
    ps = params(mlp)
    local â„’
    
    for i in 1:n_iters
        CUDA.@time begin
            img_index = rand(1:size(images, 4))
            pose = gpu(poses[:, :, img_index])

            weights = ProbabilityWeights(map((x)->gaussian_2d(x..., W/6, H/6, W, H), Tuple.(CartesianIndices((H, W))))[:])

            pixels = CartesianIndices((H, W))[sample(1:H*W, weights, batch_size, replace=false)]

            # pixels = CartesianIndices((H, W))[rand(1:H*W, batch_size)]

            target = gpu(images[:, :, :, img_index][:, pixels])

            origin, directions = get_rays(pixels, H, W, focal, pose)
            t = sample_t(directions, 2, 5, n_samples, randomized=true)
            Î¼, Î£_diag, normed_directions = cast(origin, directions, rÌ‡, t)

            local sample_weights

            gs_coarse = gradient(ps) do
                rgb, Ïƒ = mlp(IPE(Î¼, Î£_diag), posenc(normed_directions, 4))
                rgb, _, _, sample_weights = render(rgb, Ïƒ, t, directions)
                â„’ = mean((rgb .- target).^2)
                return â„’
            end
            
            t = resample_t(t, sample_weights, n_samples)
            Î¼, Î£_diag, normed_directions = cast(origin, directions, rÌ‡, t)

            gs_fine = gradient(ps) do
                rgb, Ïƒ = mlp(IPE(Î¼, Î£_diag), posenc(normed_directions, 4))
                rgb, _, _, _ = render(rgb, Ïƒ, t, directions)
                â„’ = mean((rgb .- target).^2)
                return â„’
            end

            Flux.update!(opt, ps, (0.1 .* gs_coarse) .+ gs_fine)
        end
        # println(i)
        if i%i_plot == 0
            rgbs = []
            for chunk in 0:ceil(Integer, (H*W)/batch_size)-1
                pixels = CartesianIndices((H, W))[chunk*batch_size+1:min(end, (chunk+1)*batch_size)]
                origin, directions = get_rays(pixels, H, W, focal, gpu(testpose))
                t = sample_t(directions, 2, 5, n_samples, randomized=true)
                Î¼, Î£_diag, normed_directions = cast(origin, directions, rÌ‡, t)
            
                rgb, Ïƒ = mlp(IPE(Î¼, Î£_diag), posenc(normed_directions, 4))
                rgb, _, _, weights = render(rgb, Ïƒ, t, directions)

                t = resample_t(t, weights, n_samples)
                Î¼, Î£_diag, normed_directions = cast(origin, directions, rÌ‡, t)
            
                rgb, Ïƒ = mlp(IPE(Î¼, Î£_diag), posenc(normed_directions, 4))
                rgb, _, _, _ = render(rgb, Ïƒ, t, directions)

                push!(rgbs, rgb)
            end
            rgb = reshape(reduce(hcat, rgbs), (3, H, W))
            display(colorview(RGB, cpu(rgb)))
            testing_loss = mean((rgb .- testimg) .^ 2)
            PSNR = -10 * log10(testing_loss)
            println("training loss: $â„’\ttesting loss: $testing_loss\tPSNR: $PSNR")
        end
    end
end


end
